"""
    Data rules
    - The data should be (sequence length, input dimension)
    - Sequence length = 1 ~ offline feedforward
    - num_steps_in_timeseries = 1 ~ online, else offline
        - both vacuously feedforward if sequence length = 1
    - num_examples_in_minibatch = n, for both online and offline
        - online will usually have num_examples_in_minibatch = num_examples, so it ends 
        - you can use online algorithms in a offline way. i.e. rtrl and bptt should be interchangable
        - rtrl = bptt when num_steps_to_avg_in_timeseries * num_steps_in_timeseries = sequence_length
        - while this is about how many examples in parallel, the interpretation of a batch changes
            - offline: next set of full sequence random examples i.e. 28x28 MNIST
            - online: next set of 1-length sequence, i.e the next pixel in 728x1 MNIST
            - regardless, the other entities in the batch are not part of the same sequence but entirely independent
            - offline resets in between examples
            - online doesn't reset in between examples
        - if num_examples_in_minibatch != num_examples for online, then it resets when it moves onto the next minibatch, no different than offline
        - concrete minibatch vs virtual:
            - concrete is this
            - virtual is when online splits a single sequence up into multiple updates. 
                - each split part becomes its own batch sequentially
                - for offline, its split into 1, so its always concrete
            - so shape should be (concrete batch size, virtual minibatches, num steps in time series, input dimension)
    - ***algorithms' should be agnostic to online/offline-ness. whether its successful or not just depends
    - ***dataset should all be inherently online with offline as a special case of #virtual=1
    - num_times_to_avg_in_timeseries * num_steps_in_timeseries determines the actual sequence length of a virtual minibatch
        - this assumes the underlying algorithm takes care of the actual averaging
    - train_percent is really about how many virtual minibatches one learner gets over the other

    ? I should have preset configurations for offline/online so that I don't have to remember these rules
    ? jax scan jagged arrays. to deal with padding, I will put random shit, but then pair each sequence
    with a function that maps to true output which will be used during inference and subsequently learning. 
    """
    - data loading will work double nested two prong
        - outer loop loops over purely examples indicies
            - should make a generator that samples indicies 
            - if infinite then generator is infinitely yielding
            - else it samples randomly 
        - inner loop takes these example indicies and returns a dataloader sequentially over time series 
    - the point of padding is orthogonal to actually being able to avoid jagged arrays. 
        - literally you cannot have a dataset with one dimension that is mismatched so we must reshape with pad 



I get to choose what my inference functions gradient gets computed wrt. 
for level 1, to the first interface.state
for level 2, to the second interface.state